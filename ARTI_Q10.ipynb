{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejE93sSCiO9D",
        "outputId": "f7d5575e-a312-4aab-a8fa-74f94489c171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(len(X_train), 28 * 28) / 255.0\n",
        "X_test  = X_test.reshape(len(X_test), 28 * 28) / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(y, num_classes=10):\n",
        "  Y = np.zeros((len(y), num_classes))\n",
        "  Y[np.arange(len(y)), y] = 1\n",
        "  return Y\n",
        "\n",
        "Y_train = one_hot(y_train, 10)\n",
        "Y_test  = one_hot(y_test, 10)"
      ],
      "metadata": {
        "id": "UympB_P2iVrI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet:\n",
        "  def __init__(self, sizes, activation, w_init, w_decay):\n",
        "    self.sizes = sizes\n",
        "    self.w_init = w_init\n",
        "    self.activation = activation\n",
        "    self.w_decay = w_decay\n",
        "    self.weights = []\n",
        "    self.biases = []\n",
        "\n",
        "    for i in range(1, len(sizes)):\n",
        "        if self.w_init == \"random\":\n",
        "            W = np.random.randn(sizes[i-1], sizes[i]) * 0.1\n",
        "        elif self.w_init == \"xavier\":\n",
        "            std = np.sqrt(2 / (sizes[i-1] + sizes[i]))\n",
        "            W = np.random.randn(sizes[i-1], sizes[i]) * std\n",
        "        b = np.zeros((1, sizes[i]))\n",
        "        self.weights.append(W)\n",
        "        self.biases.append(b)\n",
        "\n",
        "    self.m_W = []\n",
        "    self.m_B = []\n",
        "    self.v_W = []\n",
        "    self.v_B = []\n",
        "    self.beta = 0.9\n",
        "    self.beta2 = 0.999\n",
        "\n",
        "    for i in range(len(self.weights)):\n",
        "        self.m_W.append(np.zeros_like(self.weights[i]))\n",
        "        self.m_B.append(np.zeros_like(self.biases[i]))\n",
        "        self.v_W.append(np.zeros_like(self.weights[i]))\n",
        "        self.v_B.append(np.zeros_like(self.biases[i]))\n",
        "\n",
        "    self.a_count = 0\n",
        "    self.t = 0\n",
        "\n",
        "  def softmax(self, inputs):\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "    return probabilities\n",
        "\n",
        "  def relu(self, Z):\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "  def tanh(self, Z):\n",
        "    return np.tanh(Z)\n",
        "\n",
        "  def sigmoid(self, Z):\n",
        "    return 1 / (1 + np.exp(-Z))\n",
        "\n",
        "  def relu_derivative(self, Z):\n",
        "    return (Z > 0).astype(float)\n",
        "\n",
        "  def tanh_derivative(self, Z):\n",
        "    return 1 - np.tanh(Z) ** 2\n",
        "\n",
        "  def sigmoid_derivative(self, Z):\n",
        "    s = 1 / (1 + np.exp(-Z))\n",
        "    return s * (1 - s)\n",
        "\n",
        "  def forward(self, X):\n",
        "    self.A = [X]\n",
        "    self.Z = []\n",
        "\n",
        "    A = X\n",
        "    L = len(self.weights)\n",
        "\n",
        "    for i in range(L):\n",
        "      Z = np.dot(A, self.weights[i]) + self.biases[i]\n",
        "      self.Z.append(Z)\n",
        "\n",
        "      if i == L - 1:\n",
        "        A = self.softmax(Z)\n",
        "      else:\n",
        "        if self.activation == \"relu\":\n",
        "            A = self.relu(Z)\n",
        "        elif self.activation == \"tanh\":\n",
        "            A = self.tanh(Z)\n",
        "        elif self.activation == \"sigmoid\":\n",
        "            A = self.sigmoid(Z)\n",
        "\n",
        "      self.A.append(A)\n",
        "\n",
        "    return A\n",
        "\n",
        "  def loss(self, Y_true, Y_pred):\n",
        "    Y_pred = np.clip(Y_pred, 1e-7, 1 - 1e-7)\n",
        "    return -np.sum(Y_true * np.log(Y_pred), axis=1)\n",
        "\n",
        "  def loss_batches(self, X, Y, batch_size):\n",
        "    total_samples = X.shape[0]\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i in range(0, total_samples, batch_size):\n",
        "      Xb = X[i:i + batch_size]\n",
        "      Yb = Y[i:i + batch_size]\n",
        "      probs = self.forward(Xb)\n",
        "      total_loss += np.sum(self.loss(Yb, probs))\n",
        "\n",
        "    return total_loss / total_samples\n",
        "\n",
        "  def accuracy(self, X, Y, batch_size):\n",
        "    total_samples = X.shape[0]\n",
        "    correct = 0\n",
        "\n",
        "    for i in range(0, total_samples, batch_size):\n",
        "      X_batch = X[i:i + batch_size]\n",
        "      Y_batch = Y[i:i + batch_size]\n",
        "      probs = self.forward(X_batch)\n",
        "      y_true = np.argmax(Y_batch, axis=1)\n",
        "      y_pred = np.argmax(probs, axis=1)\n",
        "      correct += np.sum(y_true == y_pred)\n",
        "\n",
        "    return correct / total_samples\n",
        "\n",
        "  def backward(self, Y_true):\n",
        "    total_samples = Y_true.shape[0]\n",
        "    L = len(self.weights)\n",
        "\n",
        "    self.dW = [0] * L\n",
        "    self.db = [0] * L\n",
        "\n",
        "    dZ = self.A[L] - Y_true\n",
        "\n",
        "    for i in range(L - 1, -1, -1):\n",
        "      A_prev = self.A[i]\n",
        "      self.dW[i] = np.dot(A_prev.T, dZ) / total_samples\n",
        "      self.dW[i] += self.w_decay * self.weights[i]\n",
        "      self.db[i] = np.sum(dZ, axis=0, keepdims=True) / total_samples\n",
        "\n",
        "      if i > 0:\n",
        "        dA_prev = np.dot(dZ, self.weights[i].T)\n",
        "\n",
        "        if self.activation == \"relu\":\n",
        "          grad = self.relu_derivative(self.Z[i - 1])\n",
        "        elif self.activation == \"tanh\":\n",
        "          grad = self.tanh_derivative(self.Z[i - 1])\n",
        "        elif self.activation == \"sigmoid\":\n",
        "          grad = self.sigmoid_derivative(self.Z[i - 1])\n",
        "\n",
        "        dZ = dA_prev * grad\n",
        "\n",
        "  def sgd_step(self, lr):\n",
        "    for i in range(len(self.weights)):\n",
        "      self.weights[i] -= lr * self.dW[i]\n",
        "      self.biases[i]  -= lr * self.db[i]\n",
        "\n",
        "  def momentum(self, lr):\n",
        "    for i in range(len(self.weights)):\n",
        "      self.m_W[i] = self.beta * self.m_W[i] + (1 - self.beta) * self.dW[i]\n",
        "      self.m_B[i] = self.beta * self.m_B[i] + (1 - self.beta) * self.db[i]\n",
        "      self.weights[i] -= lr * self.m_W[i]\n",
        "      self.biases[i]  -= lr * self.m_B[i]\n",
        "\n",
        "  def nesterov(self, lr):\n",
        "    for i in range(len(self.weights)):\n",
        "      self.m_W[i] = self.beta * self.m_W[i] + (1 - self.beta) * self.dW[i]\n",
        "      self.m_B[i] = self.beta * self.m_B[i] + (1 - self.beta) * self.db[i]\n",
        "      self.weights[i] -= lr * (self.beta * self.m_W[i] + (1 - self.beta) * self.dW[i])\n",
        "      self.biases[i]  -= lr * (self.beta * self.m_B[i] + (1 - self.beta) * self.db[i])\n",
        "\n",
        "  def rmsprop(self, lr):\n",
        "    for i in range(len(self.weights)):\n",
        "      self.v_W[i] = self.beta * self.v_W[i] + (1 - self.beta) * (self.dW[i] ** 2)\n",
        "      self.v_B[i] = self.beta * self.v_B[i] + (1 - self.beta) * (self.db[i] ** 2)\n",
        "      self.weights[i] -= lr * self.dW[i] / (np.sqrt(self.v_W[i]) + 1e-8)\n",
        "      self.biases[i]  -= lr * self.db[i] / (np.sqrt(self.v_B[i]) + 1e-8)\n",
        "\n",
        "  def adam(self, lr, eps=1e-8):\n",
        "    self.a_count += 1\n",
        "    t = self.a_count\n",
        "\n",
        "    for i in range(len(self.weights)):\n",
        "      self.m_W[i] = self.beta * self.m_W[i] + (1 - self.beta) * self.dW[i]\n",
        "      self.m_B[i] = self.beta * self.m_B[i] + (1 - self.beta) * self.db[i]\n",
        "\n",
        "      self.v_W[i] = self.beta2 * self.v_W[i] + (1 - self.beta2) * (self.dW[i] ** 2)\n",
        "      self.v_B[i] = self.beta2 * self.v_B[i] + (1 - self.beta2) * (self.db[i] ** 2)\n",
        "\n",
        "      mW = self.m_W[i] / (1 - self.beta ** t)\n",
        "      mB = self.m_B[i] / (1 - self.beta ** t)\n",
        "      vW = self.v_W[i] / (1 - self.beta2 ** t)\n",
        "      vB = self.v_B[i] / (1 - self.beta2 ** t)\n",
        "\n",
        "      self.weights[i] -= lr * mW / (np.sqrt(vW) + eps)\n",
        "      self.biases[i]  -= lr * mB / (np.sqrt(vB) + eps)\n",
        "\n",
        "  def nadam(self, lr, eps=1e-8):\n",
        "    self.t += 1\n",
        "    t = self.t\n",
        "\n",
        "    for i in range(len(self.weights)):\n",
        "      self.m_W[i] = self.beta * self.m_W[i] + (1 - self.beta) * self.dW[i]\n",
        "      self.m_B[i] = self.beta * self.m_B[i] + (1 - self.beta) * self.db[i]\n",
        "\n",
        "      self.v_W[i] = self.beta2 * self.v_W[i] + (1 - self.beta2) * (self.dW[i] ** 2)\n",
        "      self.v_B[i] = self.beta2 * self.v_B[i] + (1 - self.beta2) * (self.db[i] ** 2)\n",
        "\n",
        "      mW = self.m_W[i] / (1 - self.beta ** t)\n",
        "      mB = self.m_B[i] / (1 - self.beta ** t)\n",
        "      vW = self.v_W[i] / (1 - self.beta2 ** t)\n",
        "      vB = self.v_B[i] / (1 - self.beta2 ** t)\n",
        "\n",
        "      mW2 = self.beta * mW + (1 - self.beta) * self.dW[i] / (1 - self.beta ** t)\n",
        "      mB2 = self.beta * mB + (1 - self.beta) * self.db[i] / (1 - self.beta ** t)\n",
        "\n",
        "      self.weights[i] -= lr * mW2 / (np.sqrt(vW) + eps)\n",
        "      self.biases[i]  -= lr * mB2 / (np.sqrt(vB) + eps)\n",
        "\n",
        "  def train(self, X_train, Y_train, X_test, Y_test, epochs, batch_size, lr, step):\n",
        "    total_samples = X_train.shape[0]\n",
        "\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    test_losses = []\n",
        "    test_accs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      idx = np.random.permutation(total_samples)\n",
        "      X_train = X_train[idx]\n",
        "      Y_train = Y_train[idx]\n",
        "\n",
        "      for i in range(0, total_samples, batch_size):\n",
        "        Xb = X_train[i:i + batch_size]\n",
        "        Yb = Y_train[i:i + batch_size]\n",
        "\n",
        "        self.forward(Xb)\n",
        "        self.backward(Yb)\n",
        "\n",
        "        if step == \"sgd\":\n",
        "          self.sgd_step(lr)\n",
        "        elif step == \"momentum\":\n",
        "          self.momentum(lr)\n",
        "        elif step == \"nesterov\":\n",
        "          self.nesterov(lr)\n",
        "        elif step == \"rmsprop\":\n",
        "          self.rmsprop(lr)\n",
        "        elif step == \"adam\":\n",
        "          self.adam(lr)\n",
        "        elif step == \"nadam\":\n",
        "          self.nadam(lr)\n",
        "\n",
        "      train_loss = self.loss_batches(X_train, Y_train, batch_size)\n",
        "      train_acc  = self.accuracy(X_train, Y_train, batch_size)\n",
        "      test_loss  = self.loss_batches(X_test, Y_test, batch_size)\n",
        "      test_acc   = self.accuracy(X_test, Y_test, batch_size)\n",
        "\n",
        "      train_losses.append(train_loss)\n",
        "      train_accs.append(train_acc)\n",
        "      test_losses.append(test_loss)\n",
        "      test_accs.append(test_acc)\n",
        "\n",
        "      # print(\"Completed\", epoch + 1, \"out of\", epochs)\n",
        "\n",
        "    return test_acc"
      ],
      "metadata": {
        "id": "nFIhxq_niXdS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sizes1 = [784, 128, 128, 128, 128, 128, 10]\n",
        "activation1 = \"tanh\"\n",
        "weight_initialisation1 = \"random\"\n",
        "weight_decay1 = 0.0005\n",
        "batch_size1 = 32\n",
        "lr1 = 0.0001\n",
        "epochs1 = 5\n",
        "step1 = \"adam\"\n",
        "model1 = NeuralNet(sizes1, activation1, weight_initialisation1, weight_decay1)\n",
        "test_acc1 = model1.train(X_train, Y_train, X_test, Y_test, epochs1, batch_size1, lr1, step1)\n",
        "print(\"Accuracy of the model with the 1st set of configurations :\", test_acc1)\n",
        "\n",
        "sizes2 = [784, 128, 128, 128, 128, 128, 10]\n",
        "activation2 = \"tanh\"\n",
        "weight_initialisation2 = \"xavier\"\n",
        "weight_decay2 = 0.0005\n",
        "batch_size2 = 16\n",
        "lr2 = 0.001\n",
        "epochs2 = 5\n",
        "step2 = \"sgd\"\n",
        "model2 = NeuralNet(sizes2, activation2, weight_initialisation2, weight_decay2)\n",
        "test_acc2 = model2.train(X_train, Y_train, X_test, Y_test, epochs2, batch_size2, lr2, step2)\n",
        "print(\"Accuracy of the model with the 2nd set of configurations :\", test_acc2)\n",
        "\n",
        "sizes3 = [784, 128, 128, 128, 128, 10]\n",
        "activation3 = \"tanh\"\n",
        "weight_initialisation3 = \"xavier\"\n",
        "weight_decay3 = 0\n",
        "batch_size3 = 64\n",
        "lr3 = 0.0001\n",
        "epochs3 = 10\n",
        "step3 = \"sgd\"\n",
        "model3 = NeuralNet(sizes3, activation3, weight_initialisation3, weight_decay3)\n",
        "test_acc3 = model3.train(X_train, Y_train, X_test, Y_test, epochs3, batch_size3, lr3, step3)\n",
        "print(\"Accuracy of the model with the 1st set of configurations :\", test_acc3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT58bYgSiZZK",
        "outputId": "90c7c3d3-f77a-4188-be2a-c9de5144e316"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model with the 1st set of configurations : 0.9651\n",
            "Accuracy of the model with the 2nd set of configurations : 0.9182\n",
            "Accuracy of the model with the 1st set of configurations : 0.7411\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MY RECOMMENDATIONS:**\n",
        "\n",
        "As instructed, I took my learnings based on extensive experimentation with one dataset (the Fashion MNIST dataset) and applied them to another dataset (the MNIST dataset). Based on the results that I’ve received, I can confirm that these learnings have indeed helped me, and the accuracies I’ve achieved in the code above are proof of that, despite being given the freedom to use only three sets of configurations.\n",
        "\n",
        "I initially had a total of eight configurations, but I removed four of them right away because they had a weight decay rate of 0.5, and through my own experimentation and the visualizations from wandb.ai, I found that higher weight decay values like that have the most negative impact on accuracy levels. From the remaining four, I selected the three that had the highest number of hidden layers because the correlation plot I developed earlier, using all the parameters, showed that the number of hidden layers had the highest correlation with accuracy levels.\n",
        "\n",
        "These are the reasons why the three sets of configurations I chose were able to achieve the following accuracies:\n",
        "1. 0.96\n",
        "2. 0.91\n",
        "3. 0.74"
      ],
      "metadata": {
        "id": "wLKuRwZwxLqP"
      }
    }
  ]
}