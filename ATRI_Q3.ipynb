{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(len(X_train), 28 * 28) / 255.0\n",
        "X_test  = X_test.reshape(len(X_test), 28 * 28) / 255.0"
      ],
      "metadata": {
        "id": "dXebtuqdswQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(y, num_classes=10):\n",
        "  Y = np.zeros((len(y), num_classes))\n",
        "  Y[np.arange(len(y)), y] = 1\n",
        "  return Y\n",
        "\n",
        "Y_train = one_hot(y_train, 10)\n",
        "Y_test  = one_hot(y_test, 10)"
      ],
      "metadata": {
        "id": "roxReyLYsyNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet:\n",
        "  def __init__(self, sizes, activation, w_init):\n",
        "    self.sizes = sizes\n",
        "    self.w_init = w_init\n",
        "    self.activation = activation\n",
        "\n",
        "    self.weights = []\n",
        "    self.biases = []\n",
        "\n",
        "    for i in range(1, len(sizes)):\n",
        "        if self.w_init == \"random\":\n",
        "            W = np.random.randn(sizes[i-1], sizes[i]) * 0.1\n",
        "        elif self.w_init == \"xavier\":\n",
        "            std = np.sqrt(2 / (sizes[i-1] + sizes[i]))\n",
        "            W = np.random.randn(sizes[i-1], sizes[i]) * std\n",
        "        b = np.zeros((1, sizes[i]))\n",
        "        self.weights.append(W)\n",
        "        self.biases.append(b)\n",
        "\n",
        "    self.m_W = []\n",
        "    self.m_B = []\n",
        "    self.v_W = []\n",
        "    self.v_B = []\n",
        "    self.beta = 0.9\n",
        "    self.beta2 = 0.999\n",
        "\n",
        "    for i in range(len(self.weights)):\n",
        "        self.m_W.append(np.zeros_like(self.weights[i]))\n",
        "        self.m_B.append(np.zeros_like(self.biases[i]))\n",
        "        self.v_W.append(np.zeros_like(self.weights[i]))\n",
        "        self.v_B.append(np.zeros_like(self.biases[i]))\n",
        "\n",
        "    self.a_count = 0\n",
        "    self.t = 0\n",
        "\n",
        "  def softmax(self, inputs):\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "    return probabilities\n",
        "\n",
        "  def relu(self, Z):\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "  def tanh(self, Z):\n",
        "    return np.tanh(Z)\n",
        "\n",
        "  def sigmoid(self, Z):\n",
        "    return 1 / (1 + np.exp(-Z))\n",
        "\n",
        "  def relu_derivative(self, Z):\n",
        "    return (Z > 0).astype(float)\n",
        "\n",
        "  def tanh_derivative(self, Z):\n",
        "    return 1 - np.tanh(Z) ** 2\n",
        "\n",
        "  def sigmoid_derivative(self, Z):\n",
        "    s = 1 / (1 + np.exp(-Z))\n",
        "    return s * (1 - s)\n",
        "\n",
        "  def forward(self, X):\n",
        "    self.A = [X]\n",
        "    self.Z = []\n",
        "\n",
        "    A = X\n",
        "    L = len(self.weights)\n",
        "\n",
        "    for i in range(L):\n",
        "      Z = np.dot(A, self.weights[i]) + self.biases[i]\n",
        "      self.Z.append(Z)\n",
        "\n",
        "      if i == L - 1:\n",
        "        A = self.softmax(Z)\n",
        "      else:\n",
        "        if self.activation == \"relu\":\n",
        "            A = self.relu(Z)\n",
        "        elif self.activation == \"tanh\":\n",
        "            A = self.tanh(Z)\n",
        "        elif self.activation == \"sigmoid\":\n",
        "            A = self.sigmoid(Z)\n",
        "\n",
        "      self.A.append(A)\n",
        "\n",
        "    return A\n",
        "\n",
        "  def loss(self, Y_true, Y_pred):\n",
        "    Y_pred = np.clip(Y_pred, 1e-7, 1 - 1e-7)\n",
        "    return -np.sum(Y_true * np.log(Y_pred), axis=1)\n",
        "\n",
        "  def loss_batches(self, X, Y, batch_size):\n",
        "    total_samples = X.shape[0]\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i in range(0, total_samples, batch_size):\n",
        "      Xb = X[i:i + batch_size]\n",
        "      Yb = Y[i:i + batch_size]\n",
        "      probs = self.forward(Xb)\n",
        "      total_loss += np.sum(self.loss(Yb, probs))\n",
        "\n",
        "    return total_loss / total_samples\n",
        "\n",
        "  def accuracy(self, X, Y, batch_size):\n",
        "    total_samples = X.shape[0]\n",
        "    correct = 0\n",
        "\n",
        "    for i in range(0, total_samples, batch_size):\n",
        "      X_batch = X[i:i + batch_size]\n",
        "      Y_batch = Y[i:i + batch_size]\n",
        "      probs = self.forward(X_batch)\n",
        "      y_true = np.argmax(Y_batch, axis=1)\n",
        "      y_pred = np.argmax(probs, axis=1)\n",
        "      correct += np.sum(y_true == y_pred)\n",
        "\n",
        "    return correct / total_samples\n",
        "\n",
        "  def backward(self, Y_true):\n",
        "    total_samples = Y_true.shape[0]\n",
        "    L = len(self.weights)\n",
        "\n",
        "    self.dW = [0] * L\n",
        "    self.db = [0] * L\n",
        "\n",
        "    dZ = self.A[L] - Y_true\n",
        "\n",
        "    for i in range(L - 1, -1, -1):\n",
        "      A_prev = self.A[i]\n",
        "      self.dW[i] = np.dot(A_prev.T, dZ) / total_samples\n",
        "      self.db[i] = np.sum(dZ, axis=0, keepdims=True) / total_samples\n",
        "\n",
        "      if i > 0:\n",
        "        dA_prev = np.dot(dZ, self.weights[i].T)\n",
        "\n",
        "        if self.activation == \"relu\":\n",
        "          grad = self.relu_derivative(self.Z[i - 1])\n",
        "        elif self.activation == \"tanh\":\n",
        "          grad = self.tanh_derivative(self.Z[i - 1])\n",
        "        elif self.activation == \"sigmoid\":\n",
        "          grad = self.sigmoid_derivative(self.Z[i - 1])\n",
        "\n",
        "        dZ = dA_prev * grad\n",
        "\n",
        "  def sgd_step(self, lr):\n",
        "    for i in range(len(self.weights)):\n",
        "      self.weights[i] -= lr * self.dW[i]\n",
        "      self.biases[i]  -= lr * self.db[i]\n",
        "\n",
        "  def momentum(self, lr):\n",
        "    for i in range(len(self.weights)):\n",
        "      self.m_W[i] = self.beta * self.m_W[i] + (1 - self.beta) * self.dW[i]\n",
        "      self.m_B[i] = self.beta * self.m_B[i] + (1 - self.beta) * self.db[i]\n",
        "      self.weights[i] -= lr * self.m_W[i]\n",
        "      self.biases[i]  -= lr * self.m_B[i]\n",
        "\n",
        "  def nesterov(self, lr):\n",
        "    for i in range(len(self.weights)):\n",
        "      self.m_W[i] = self.beta * self.m_W[i] + (1 - self.beta) * self.dW[i]\n",
        "      self.m_B[i] = self.beta * self.m_B[i] + (1 - self.beta) * self.db[i]\n",
        "      self.weights[i] -= lr * (self.beta * self.m_W[i] + (1 - self.beta) * self.dW[i])\n",
        "      self.biases[i]  -= lr * (self.beta * self.m_B[i] + (1 - self.beta) * self.db[i])\n",
        "\n",
        "  def rmsprop(self, lr):\n",
        "    for i in range(len(self.weights)):\n",
        "      self.v_W[i] = self.beta * self.v_W[i] + (1 - self.beta) * (self.dW[i] ** 2)\n",
        "      self.v_B[i] = self.beta * self.v_B[i] + (1 - self.beta) * (self.db[i] ** 2)\n",
        "      self.weights[i] -= lr * self.dW[i] / (np.sqrt(self.v_W[i]) + 1e-8)\n",
        "      self.biases[i]  -= lr * self.db[i] / (np.sqrt(self.v_B[i]) + 1e-8)\n",
        "\n",
        "  def adam(self, lr, eps=1e-8):\n",
        "    self.a_count += 1\n",
        "    t = self.a_count\n",
        "\n",
        "    for i in range(len(self.weights)):\n",
        "      self.m_W[i] = self.beta * self.m_W[i] + (1 - self.beta) * self.dW[i]\n",
        "      self.m_B[i] = self.beta * self.m_B[i] + (1 - self.beta) * self.db[i]\n",
        "\n",
        "      self.v_W[i] = self.beta2 * self.v_W[i] + (1 - self.beta2) * (self.dW[i] ** 2)\n",
        "      self.v_B[i] = self.beta2 * self.v_B[i] + (1 - self.beta2) * (self.db[i] ** 2)\n",
        "\n",
        "      mW = self.m_W[i] / (1 - self.beta ** t)\n",
        "      mB = self.m_B[i] / (1 - self.beta ** t)\n",
        "      vW = self.v_W[i] / (1 - self.beta2 ** t)\n",
        "      vB = self.v_B[i] / (1 - self.beta2 ** t)\n",
        "\n",
        "      self.weights[i] -= lr * mW / (np.sqrt(vW) + eps)\n",
        "      self.biases[i]  -= lr * mB / (np.sqrt(vB) + eps)\n",
        "\n",
        "  def nadam(self, lr, eps=1e-8):\n",
        "    self.t += 1\n",
        "    t = self.t\n",
        "\n",
        "    for i in range(len(self.weights)):\n",
        "      self.m_W[i] = self.beta * self.m_W[i] + (1 - self.beta) * self.dW[i]\n",
        "      self.m_B[i] = self.beta * self.m_B[i] + (1 - self.beta) * self.db[i]\n",
        "\n",
        "      self.v_W[i] = self.beta2 * self.v_W[i] + (1 - self.beta2) * (self.dW[i] ** 2)\n",
        "      self.v_B[i] = self.beta2 * self.v_B[i] + (1 - self.beta2) * (self.db[i] ** 2)\n",
        "\n",
        "      mW = self.m_W[i] / (1 - self.beta ** t)\n",
        "      mB = self.m_B[i] / (1 - self.beta ** t)\n",
        "      vW = self.v_W[i] / (1 - self.beta2 ** t)\n",
        "      vB = self.v_B[i] / (1 - self.beta2 ** t)\n",
        "\n",
        "      mW2 = self.beta * mW + (1 - self.beta) * self.dW[i] / (1 - self.beta ** t)\n",
        "      mB2 = self.beta * mB + (1 - self.beta) * self.db[i] / (1 - self.beta ** t)\n",
        "\n",
        "      self.weights[i] -= lr * mW2 / (np.sqrt(vW) + eps)\n",
        "      self.biases[i]  -= lr * mB2 / (np.sqrt(vB) + eps)\n",
        "\n",
        "  def train(self, X_train, Y_train, X_test, Y_test, epochs, batch_size, lr, step):\n",
        "    total_samples = X_train.shape[0]\n",
        "\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    test_losses = []\n",
        "    test_accs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      idx = np.random.permutation(total_samples)\n",
        "      X_train = X_train[idx]\n",
        "      Y_train = Y_train[idx]\n",
        "\n",
        "      for i in range(0, total_samples, batch_size):\n",
        "        Xb = X_train[i:i + batch_size]\n",
        "        Yb = Y_train[i:i + batch_size]\n",
        "\n",
        "        self.forward(Xb)\n",
        "        self.backward(Yb)\n",
        "\n",
        "        if step == \"sgd\":\n",
        "          self.sgd_step(lr)\n",
        "        elif step == \"momentum\":\n",
        "          self.momentum(lr)\n",
        "        elif step == \"nesterov\":\n",
        "          self.nesterov(lr)\n",
        "        elif step == \"rmsprop\":\n",
        "          self.rmsprop(lr)\n",
        "        elif step == \"adam\":\n",
        "          self.adam(lr)\n",
        "        elif step == \"nadam\":\n",
        "          self.nadam(lr)\n",
        "\n",
        "      train_loss = self.loss_batches(X_train, Y_train, batch_size)\n",
        "      train_acc  = self.accuracy(X_train, Y_train, batch_size)\n",
        "      test_loss  = self.loss_batches(X_test, Y_test, batch_size)\n",
        "      test_acc   = self.accuracy(X_test, Y_test, batch_size)\n",
        "\n",
        "      train_losses.append(train_loss)\n",
        "      train_accs.append(train_acc)\n",
        "      test_losses.append(test_loss)\n",
        "      test_accs.append(test_acc)\n",
        "\n",
        "      print(f\"\\nFor epoch {epoch+1}:\\n\"\n",
        "        f\"Train loss = {train_loss:.4f}\\n\"\n",
        "        f\"Train acc  = {train_acc:.4f}\\n\"\n",
        "        f\"Test loss  = {test_loss:.4f}\\n\"\n",
        "        f\"Test acc   = {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "hM6p3aBbs61z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sizes = [784, 128, 64, 10]\n",
        "activation = \"relu\"\n",
        "weight_initialisation = \"xavier\"\n",
        "model = NeuralNet(sizes, activation, weight_initialisation)\n",
        "\n",
        "batch_size = 256\n",
        "lr = 0.1\n",
        "epochs = 5\n",
        "step = \"sgd\"\n",
        "\n",
        "model.train(X_train, Y_train, X_test, Y_test, epochs, batch_size, lr, step)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFuj_3JVtJBr",
        "outputId": "5db1bc01-eb51-4d0a-d2af-84edec230280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "For epoch 1:\n",
            "Train loss = 0.5507\n",
            "Train acc  = 0.7976\n",
            "Test loss  = 0.5780\n",
            "Test acc   = 0.7848\n",
            "\n",
            "For epoch 2:\n",
            "Train loss = 0.4783\n",
            "Train acc  = 0.8287\n",
            "Test loss  = 0.5123\n",
            "Test acc   = 0.8109\n",
            "\n",
            "For epoch 3:\n",
            "Train loss = 0.4466\n",
            "Train acc  = 0.8386\n",
            "Test loss  = 0.4794\n",
            "Test acc   = 0.8305\n",
            "\n",
            "For epoch 4:\n",
            "Train loss = 0.5128\n",
            "Train acc  = 0.8010\n",
            "Test loss  = 0.5593\n",
            "Test acc   = 0.7781\n",
            "\n",
            "For epoch 5:\n",
            "Train loss = 0.3935\n",
            "Train acc  = 0.8564\n",
            "Test loss  = 0.4359\n",
            "Test acc   = 0.8386\n"
          ]
        }
      ]
    }
  ]
}