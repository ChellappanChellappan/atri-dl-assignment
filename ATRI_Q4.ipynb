{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFMd_c89wnLz",
        "outputId": "9dcb5d94-5afa-4355-dcc8-88da6528b68b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.24.0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.46)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.50.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2026.1.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "import wandb"
      ],
      "metadata": {
        "id": "kAgc3mFY9tA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_split(X, y, val_percent, seed=42):\n",
        "  np.random.seed(seed)\n",
        "  total_samples = len(X)\n",
        "  total_val = int((val_percent / 100) * total_samples)\n",
        "\n",
        "  idx = np.random.permutation(total_samples)\n",
        "  X = X[idx]\n",
        "  y = y[idx]\n",
        "\n",
        "  X_val = X[:total_val]\n",
        "  y_val = y[:total_val]\n",
        "\n",
        "  X_train = X[total_val:]\n",
        "  y_train = y[total_val:]\n",
        "\n",
        "  return X_train, y_train, X_val, y_val\n",
        "\n",
        "\n",
        "(X_train_1, y_train_1), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "X_train_1 = X_train_1.reshape(len(X_train_1), 28 * 28) / 255.0\n",
        "X_test  = X_test.reshape(len(X_test), 28 * 28) / 255.0\n",
        "val_percentage = 10\n",
        "X_train, y_train, X_val, y_val = train_val_split(X_train_1, y_train_1, val_percentage)"
      ],
      "metadata": {
        "id": "HYQG7ceLCIoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(y, num_classes=10):\n",
        "  Y = np.zeros((len(y), num_classes))\n",
        "  Y[np.arange(len(y)), y] = 1\n",
        "  return Y\n",
        "\n",
        "Y_train = one_hot(y_train, 10)\n",
        "Y_val   = one_hot(y_val, 10)\n",
        "Y_test  = one_hot(y_test, 10)"
      ],
      "metadata": {
        "id": "z8KMMiMCwYZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet:\n",
        "  def __init__(self, sizes, activation, w_init, w_decay):\n",
        "    self.sizes = sizes\n",
        "    self.w_init = w_init\n",
        "    self.activation = activation\n",
        "    self.w_decay = w_decay\n",
        "\n",
        "    self.weights = []\n",
        "    self.biases = []\n",
        "\n",
        "    for i in range(1, len(sizes)):\n",
        "        if self.w_init == \"random\":\n",
        "            W = np.random.randn(sizes[i-1], sizes[i]) * 0.1\n",
        "        elif self.w_init == \"xavier\":\n",
        "            std = np.sqrt(2 / (sizes[i-1] + sizes[i]))\n",
        "            W = np.random.randn(sizes[i-1], sizes[i]) * std\n",
        "        b = np.zeros((1, sizes[i]))\n",
        "        self.weights.append(W)\n",
        "        self.biases.append(b)\n",
        "\n",
        "    self.m_W = []\n",
        "    self.m_B = []\n",
        "    self.v_W = []\n",
        "    self.v_B = []\n",
        "    self.beta = 0.9\n",
        "    self.beta2 = 0.999\n",
        "\n",
        "    for i in range(len(self.weights)):\n",
        "        self.m_W.append(np.zeros_like(self.weights[i]))\n",
        "        self.m_B.append(np.zeros_like(self.biases[i]))\n",
        "        self.v_W.append(np.zeros_like(self.weights[i]))\n",
        "        self.v_B.append(np.zeros_like(self.biases[i]))\n",
        "\n",
        "    self.a_count = 0\n",
        "    self.t = 0\n",
        "\n",
        "  def softmax(self, inputs):\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "    return probabilities\n",
        "\n",
        "  def relu(self, Z):\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "  def tanh(self, Z):\n",
        "    return np.tanh(Z)\n",
        "\n",
        "  def sigmoid(self, Z):\n",
        "    return 1 / (1 + np.exp(-Z))\n",
        "\n",
        "  def relu_derivative(self, Z):\n",
        "    return (Z > 0).astype(float)\n",
        "\n",
        "  def tanh_derivative(self, Z):\n",
        "    return 1 - np.tanh(Z) ** 2\n",
        "\n",
        "  def sigmoid_derivative(self, Z):\n",
        "    s = 1 / (1 + np.exp(-Z))\n",
        "    return s * (1 - s)\n",
        "\n",
        "  def forward(self, X):\n",
        "    self.A = [X]\n",
        "    self.Z = []\n",
        "\n",
        "    A = X\n",
        "    L = len(self.weights)\n",
        "\n",
        "    for i in range(L):\n",
        "      Z = np.dot(A, self.weights[i]) + self.biases[i]\n",
        "      self.Z.append(Z)\n",
        "\n",
        "      if i == L - 1:\n",
        "        A = self.softmax(Z)\n",
        "      else:\n",
        "        if self.activation == \"relu\":\n",
        "            A = self.relu(Z)\n",
        "        elif self.activation == \"tanh\":\n",
        "            A = self.tanh(Z)\n",
        "        elif self.activation == \"sigmoid\":\n",
        "            A = self.sigmoid(Z)\n",
        "\n",
        "      self.A.append(A)\n",
        "\n",
        "    return A\n",
        "\n",
        "  def loss(self, Y_true, Y_pred):\n",
        "    Y_pred = np.clip(Y_pred, 1e-7, 1 - 1e-7)\n",
        "    return -np.sum(Y_true * np.log(Y_pred), axis=1)\n",
        "\n",
        "  def loss_batches(self, X, Y, batch_size):\n",
        "    total_samples = X.shape[0]\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i in range(0, total_samples, batch_size):\n",
        "      Xb = X[i:i + batch_size]\n",
        "      Yb = Y[i:i + batch_size]\n",
        "      probs = self.forward(Xb)\n",
        "      total_loss += np.sum(self.loss(Yb, probs))\n",
        "\n",
        "    return total_loss / total_samples\n",
        "\n",
        "  def accuracy(self, X, Y, batch_size):\n",
        "    total_samples = X.shape[0]\n",
        "    correct = 0\n",
        "\n",
        "    for i in range(0, total_samples, batch_size):\n",
        "      X_batch = X[i:i + batch_size]\n",
        "      Y_batch = Y[i:i + batch_size]\n",
        "      probs = self.forward(X_batch)\n",
        "      y_true = np.argmax(Y_batch, axis=1)\n",
        "      y_pred = np.argmax(probs, axis=1)\n",
        "      correct += np.sum(y_true == y_pred)\n",
        "\n",
        "    return correct / total_samples\n",
        "\n",
        "  def backward(self, Y_true):\n",
        "    total_samples = Y_true.shape[0]\n",
        "    L = len(self.weights)\n",
        "\n",
        "    self.dW = [0] * L\n",
        "    self.db = [0] * L\n",
        "\n",
        "    dZ = self.A[L] - Y_true\n",
        "\n",
        "    for i in range(L - 1, -1, -1):\n",
        "      A_prev = self.A[i]\n",
        "      self.dW[i] = np.dot(A_prev.T, dZ) / total_samples\n",
        "      self.dW[i] += self.w_decay * self.weights[i]\n",
        "      self.db[i] = np.sum(dZ, axis=0, keepdims=True) / total_samples\n",
        "\n",
        "      if i > 0:\n",
        "        dA_prev = np.dot(dZ, self.weights[i].T)\n",
        "\n",
        "        if self.activation == \"relu\":\n",
        "          grad = self.relu_derivative(self.Z[i - 1])\n",
        "        elif self.activation == \"tanh\":\n",
        "          grad = self.tanh_derivative(self.Z[i - 1])\n",
        "        elif self.activation == \"sigmoid\":\n",
        "          grad = self.sigmoid_derivative(self.Z[i - 1])\n",
        "\n",
        "        dZ = dA_prev * grad\n",
        "\n",
        "  def sgd_step(self, lr):\n",
        "    for i in range(len(self.weights)):\n",
        "      self.weights[i] -= lr * self.dW[i]\n",
        "      self.biases[i]  -= lr * self.db[i]\n",
        "\n",
        "  def momentum(self, lr):\n",
        "    for i in range(len(self.weights)):\n",
        "      self.m_W[i] = self.beta * self.m_W[i] + (1 - self.beta) * self.dW[i]\n",
        "      self.m_B[i] = self.beta * self.m_B[i] + (1 - self.beta) * self.db[i]\n",
        "      self.weights[i] -= lr * self.m_W[i]\n",
        "      self.biases[i]  -= lr * self.m_B[i]\n",
        "\n",
        "  def nesterov(self, lr):\n",
        "    for i in range(len(self.weights)):\n",
        "      self.m_W[i] = self.beta * self.m_W[i] + (1 - self.beta) * self.dW[i]\n",
        "      self.m_B[i] = self.beta * self.m_B[i] + (1 - self.beta) * self.db[i]\n",
        "      self.weights[i] -= lr * (self.beta * self.m_W[i] + (1 - self.beta) * self.dW[i])\n",
        "      self.biases[i]  -= lr * (self.beta * self.m_B[i] + (1 - self.beta) * self.db[i])\n",
        "\n",
        "  def rmsprop(self, lr):\n",
        "    for i in range(len(self.weights)):\n",
        "      self.v_W[i] = self.beta * self.v_W[i] + (1 - self.beta) * (self.dW[i] ** 2)\n",
        "      self.v_B[i] = self.beta * self.v_B[i] + (1 - self.beta) * (self.db[i] ** 2)\n",
        "      self.weights[i] -= lr * self.dW[i] / (np.sqrt(self.v_W[i]) + 1e-8)\n",
        "      self.biases[i]  -= lr * self.db[i] / (np.sqrt(self.v_B[i]) + 1e-8)\n",
        "\n",
        "  def adam(self, lr, eps=1e-8):\n",
        "    self.a_count += 1\n",
        "    t = self.a_count\n",
        "\n",
        "    for i in range(len(self.weights)):\n",
        "      self.m_W[i] = self.beta * self.m_W[i] + (1 - self.beta) * self.dW[i]\n",
        "      self.m_B[i] = self.beta * self.m_B[i] + (1 - self.beta) * self.db[i]\n",
        "\n",
        "      self.v_W[i] = self.beta2 * self.v_W[i] + (1 - self.beta2) * (self.dW[i] ** 2)\n",
        "      self.v_B[i] = self.beta2 * self.v_B[i] + (1 - self.beta2) * (self.db[i] ** 2)\n",
        "\n",
        "      mW = self.m_W[i] / (1 - self.beta ** t)\n",
        "      mB = self.m_B[i] / (1 - self.beta ** t)\n",
        "      vW = self.v_W[i] / (1 - self.beta2 ** t)\n",
        "      vB = self.v_B[i] / (1 - self.beta2 ** t)\n",
        "\n",
        "      self.weights[i] -= lr * mW / (np.sqrt(vW) + eps)\n",
        "      self.biases[i]  -= lr * mB / (np.sqrt(vB) + eps)\n",
        "\n",
        "  def nadam(self, lr, eps=1e-8):\n",
        "    self.t += 1\n",
        "    t = self.t\n",
        "\n",
        "    for i in range(len(self.weights)):\n",
        "      self.m_W[i] = self.beta * self.m_W[i] + (1 - self.beta) * self.dW[i]\n",
        "      self.m_B[i] = self.beta * self.m_B[i] + (1 - self.beta) * self.db[i]\n",
        "\n",
        "      self.v_W[i] = self.beta2 * self.v_W[i] + (1 - self.beta2) * (self.dW[i] ** 2)\n",
        "      self.v_B[i] = self.beta2 * self.v_B[i] + (1 - self.beta2) * (self.db[i] ** 2)\n",
        "\n",
        "      mW = self.m_W[i] / (1 - self.beta ** t)\n",
        "      mB = self.m_B[i] / (1 - self.beta ** t)\n",
        "      vW = self.v_W[i] / (1 - self.beta2 ** t)\n",
        "      vB = self.v_B[i] / (1 - self.beta2 ** t)\n",
        "\n",
        "      mW2 = self.beta * mW + (1 - self.beta) * self.dW[i] / (1 - self.beta ** t)\n",
        "      mB2 = self.beta * mB + (1 - self.beta) * self.db[i] / (1 - self.beta ** t)\n",
        "\n",
        "      self.weights[i] -= lr * mW2 / (np.sqrt(vW) + eps)\n",
        "      self.biases[i]  -= lr * mB2 / (np.sqrt(vB) + eps)"
      ],
      "metadata": {
        "id": "agCciA6Hwcc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "  wandb.init()\n",
        "  config = wandb.config\n",
        "  hidden_layers = config.hidden_layers\n",
        "  hidden_size = config.hidden_size\n",
        "\n",
        "  sizes = [784]\n",
        "  for _ in range(hidden_layers):\n",
        "    sizes.append(hidden_size)\n",
        "  sizes.append(10)\n",
        "\n",
        "  epochs = config.epochs\n",
        "  batch_size = config.batch_size\n",
        "  lr = config.lr\n",
        "  activation = config.activation\n",
        "  weight_init = config.weight_init\n",
        "  opt = config.optimizer\n",
        "  weight_decay = config.weight_decay\n",
        "  model = NeuralNet(sizes, activation, weight_init, weight_decay)\n",
        "  total_samples = X_train.shape[0]\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    idx = np.random.permutation(total_samples)\n",
        "    X_t = X_train[idx]\n",
        "    Y_t = Y_train[idx]\n",
        "\n",
        "    for i in range(0, total_samples, batch_size):\n",
        "      Xb = X_t[i:i + batch_size]\n",
        "      Yb = Y_t[i:i + batch_size]\n",
        "\n",
        "      model.forward(Xb)\n",
        "      model.backward(Yb)\n",
        "\n",
        "      if opt == \"sgd\":\n",
        "        model.sgd_step(lr)\n",
        "      elif opt == \"momentum\":\n",
        "        model.momentum(lr)\n",
        "      elif opt == \"nesterov\":\n",
        "        model.nesterov(lr)\n",
        "      elif opt == \"rmsprop\":\n",
        "        model.rmsprop(lr)\n",
        "      elif opt == \"adam\":\n",
        "        model.adam(lr)\n",
        "      elif opt == \"nadam\":\n",
        "        model.nadam(lr)\n",
        "\n",
        "    train_loss = model.loss_batches(X_train, Y_train, batch_size)\n",
        "    train_acc  = model.accuracy(X_train, Y_train, batch_size)\n",
        "    val_loss  = model.loss_batches(X_val, Y_val, batch_size)\n",
        "    val_acc   = model.accuracy(X_val, Y_val, batch_size)\n",
        "\n",
        "    wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_acc\": train_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_acc\": val_acc\n",
        "        })"
      ],
      "metadata": {
        "id": "8OqI6R2xDPc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\n",
        "        \"name\": \"val_acc\",\n",
        "        \"goal\": \"maximize\"\n",
        "        },\n",
        "    \"parameters\": {\n",
        "        \"epochs\": {\n",
        "            \"values\": [5, 10]\n",
        "            },\n",
        "        \"hidden_layers\": {\n",
        "            \"values\": [3, 4, 5]\n",
        "            },\n",
        "        \"hidden_size\": {\n",
        "            \"values\": [32, 64, 128]\n",
        "            },\n",
        "        \"lr\": {\n",
        "            \"values\": [1e-3, 1e-4]\n",
        "            },\n",
        "        \"optimizer\": {\n",
        "            \"values\": [\"sgd\", \"momentum\", \"nesterov\", \"rmsprop\", \"adam\", \"nadam\"]\n",
        "            },\n",
        "        \"batch_size\": {\n",
        "            \"values\": [16, 32, 64]\n",
        "            },\n",
        "        \"weight_init\": {\n",
        "            \"values\": [\"random\", \"xavier\"]\n",
        "            },\n",
        "        \"activation\": {\n",
        "            \"values\": [\"sigmoid\", \"tanh\", \"relu\"]\n",
        "            },\n",
        "        \"weight_decay\": {\n",
        "            \"values\": [0.0, 0.0005, 0.5]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "wandb.login()\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"Fashion MNIST Hyperparameter Tuning with wandb\")\n",
        "wandb.agent(sweep_id, train, count = 8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "LHKLqNysweVs",
        "outputId": "0e822540-b303-49f6-d377-5aff5719d70d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: ig26q0pw\n",
            "Sweep URL: https://wandb.ai/cl-chellappan-atri-ai/Fashion%20MNIST%20Hyperparameter%20Tuning%20with%20wandb/sweeps/ig26q0pw\n"
          ]
        }
      ]
    }
  ]
}